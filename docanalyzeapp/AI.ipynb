{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm.auto import tqdm, trange\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta-ru-news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[:800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['tags'].unique(), len(data['tags'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['topic'].unique(), len(data['topic'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выборка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    'Кино', 'Преступность', 'Общество', 'Происшествия',\n",
    "    'Искусство', 'Бизнес', 'Техника', 'ТВ и радио', 'Политика',\n",
    "    'Пресса', 'Музыка', 'Люди', 'Звери', 'Игры', 'Культура'\n",
    "    'Гаджеты', 'Наука', 'Еда', 'Рынки', 'Деньги', 'Интернет',\n",
    "    'Театр', 'Реклама', 'Космос', 'Бокс и ММА', 'Футбол', 'Книги',\n",
    "    'Достижения', 'Coцсети', 'События', 'Софт', 'Квартира', 'Город', 'Дача',\n",
    "    'Офис', 'Оружие', 'Криминал',  'Мемы', 'Инновации', 'Хоккей',\n",
    "    'Стиль', 'Инструменты', 'История', 'Туризм', 'Экология',\n",
    "    'Фотография', 'Авто', 'Жизнь', 'Киберпреступность', 'Экономика',\n",
    "]\n",
    "news_in_cat_count = 2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame()\n",
    "\n",
    "for tag in tqdm(tags):\n",
    "    df_topic = data[data['tags'] == tag][:news_in_cat_count]\n",
    "    df_res = df_res.append(df_topic, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "import re\n",
    "def remove_multiple_spaces(text):\n",
    "\treturn re.sub(r'\\s+', ' ', text, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "mystem = Mystem()\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['…', '«', '»', '...'])\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [\n",
    "        token for token in tokens if token not in russian_stopwords and token != \" \"]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccessing = lambda text: (remove_multiple_spaces(remove_numbers(remove_punctuation(text))))\n",
    "data['preproccessed'] = list(map(preproccessing, df_res['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_text = [remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower()))) for text in tqdm(df_res['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prep_text)\n",
    "prep_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['text_prep'] = prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['…', '«', '»', '...', 'т.д.', 'т', 'д'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_res['text_prep'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "stemmed_texts_list = []\n",
    "for text in tqdm(df_res['text_prep']):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in russian_stopwords]\n",
    "    text = \" \".join(stemmed_tokens)\n",
    "    stemmed_texts_list.append(text)\n",
    "\n",
    "df_res['text_stem'] = stemmed_texts_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text) \n",
    "    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sw_texts_list = []\n",
    "for text in tqdm(df_res['text_prep']):\n",
    "    tokens = word_tokenize(text)    \n",
    "    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']\n",
    "    text = \" \".join(tokens)\n",
    "    sw_texts_list.append(text)\n",
    "\n",
    "df_res['text_sw'] = sw_texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['text_sw'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv('lenta_stemmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['text_stem'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv('lenta_stemmed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_texts_list = []\n",
    "for text in tqdm(df_res['text_sw']):\n",
    "    #print(text)\n",
    "    try:\n",
    "        text_lem = mystem.lemmatize(text)\n",
    "        tokens = [token for token in text_lem if token != ' ' and token not in russian_stopwords]\n",
    "        text = \" \".join(tokens)\n",
    "        lemm_texts_list.append(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "df_res['text_lemm'] = lemm_texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text_lem = mystem.lemmatize(text)\n",
    "    tokens = [token for token in text_lem if token != ' ']\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv('lemm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv('lemm.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['text_lemm'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_res['text_lemm']\n",
    "X = df_res['text_sw']\n",
    "y = df_res['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_res['text_lemm'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = df_res['tags'].unique()\n",
    "my_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train[1])\n",
    "df['Review'].values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(nb,'native_bayes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[3000], y_test[3000], y_pred[3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=my_tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sgd,'sgd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(logreg,'logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на примерах новостей с сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_text = '''\n",
    "Доллар США считается символом надежности, безопасности и экономического процветания. \n",
    "Он занимает неоспоримое доминирующее положение в международной финансовой системе с середины XX века и производит \n",
    "впечатление непобедимого титана. Однако эра господства доллара как основной мировой резервной валюты медленно \n",
    "подходит к концу. Крупнейшие банки предрекают ему резкий спад уже в следующем году, а известный экономист \n",
    "Стивен Роуч уверен, что американская валюта может обесцениться на треть. \n",
    "Причинами обвала станут сокращение сбережений населения, рост государственного долга США и усиление Китая. \n",
    "Закат долларового диктата — в материале «Ленты.ру».\n",
    "Непомерные привилегии\n",
    "\n",
    "Успех американской экономики в XX веке был во многом обусловлен доминирующей ролью доллара. В свою очередь достижение этой роли стало результатом политического и военного превосходства, которое США приобрели после Первой мировой войны. До сих пор позиция доллара в мире финансов представляет собой главную основу процветания США. Однако в 2002 году наметилась долгосрочная тенденция к ослаблению американской валюты, которая наблюдается и по сей день.\n",
    "\n",
    "Действительно, последнее время доллар чувствует себя не очень хорошо. В частности, июль оказался очень сложным месяцем для американской валюты, которая обновила многомесячные, а в некоторых случаях и многолетние минимумы. Всего за месяц доллар подешевел на шесть процентов против фунта, на пять — по отношению к евро, на четыре — против швейцарского франка и австралийского доллара. Вдобавок валютные аналитики вполне допускают, что июль может оказаться для доллара худшим месяцем с точки зрения месячной динамики за последние десять лет.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_text = remove_multiple_spaces(remove_numbers(remove_punctuation(econ_text.lower())))\n",
    "econ_text = remove_stop_words(econ_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "econ_text = lemmatize_text(econ_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ect_pred1 = nb.predict([econ_text])\n",
    "ect_pred2 = sgd.predict([econ_text])\n",
    "ect_pred3 = logreg.predict([econ_text])\n",
    "print(ect_pred1, ect_pred2, ect_pred3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_text = '''\n",
    "Ученые Университета Аризоны в США открыли неизвестный принцип эволюции, согласно которому естественный отбор не может благоприятствовать сразу нескольким полезным мутациям, повышающим приспособленность организма к окружающей среде. Вместо этого он фокусируется только на том, что достаточно для выживания существа, и игнорирует другие возможности для улучшения. Об этом сообщается в статье в журнале Proceedings of the National Academy of Sciences.\n",
    "Исследователи спровоцировали несколько мутаций, которые «ломали» механизм трансляции у шести различных штаммов бактерии кишечной палочки Escherichia coli. Трансляция — это процесс, при котором заложенная в генах информация о нуклеотидной последовательности белков переносится к рибосомам и участвует в белковом синтезе. Известно, что более чем за 3,5 миллиарда лет эволюции механизм трансляции не претерпел существенных изменений.\n",
    "Ряд нефатальных мутаций, затрагивающих функционирование трансляционного процесса, должен с течением времени быть исправлен эволюцией. Для этого ученые заставили штаммы бактерий с различными изменениями конкурировать друг с другом, благодаря чему осуществлялся естественный отбор. Со временем внутри популяции накапливались полезные мутации, которые могли быть сохранены и использованы для улучшения трансляции.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_text = remove_multiple_spaces(remove_numbers(remove_punctuation(tech_text.lower())))\n",
    "tech_text = remove_stop_words(tech_text)\n",
    "#tech_text = lemmatize_text(tech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_pred1 = nb.predict([tech_text])\n",
    "tech_pred2 = sgd.predict([tech_text])\n",
    "tech_pred3 = logreg.predict([tech_text])\n",
    "print(tech_pred1, tech_pred2, tech_pred3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_text = '''\n",
    "Российские туристы с 1 августа могут беспрепятственно въезжать в Абхазию, а граждане этой страны — в Россию, соответствующее распоряжение об открытии границ между странами подписал премьер-министр России Михаил Мишустин.\n",
    "Границу уже свободно пересекают пешеходы и транспорт, все введенные ранее на фоне пандемии коронавируса ограничения сняты.\n",
    "При этом о своем намерении российские власти сообщили еще в пятницу, 31 июля — решение было принято по итогам заседания оперативного штаба по борьбе с распространением инфекции на основании заключений Роспотребнадзора и Минздрава России по оценке санитарно-эпидемиологической обстановки в стране, а также определения готовности объектов абхазской инфраструктуры к приему туристов.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_text = remove_multiple_spaces(remove_numbers(remove_punctuation(travel_text.lower())))\n",
    "travel_text = remove_stop_words(travel_text)\n",
    "#travel_text = lemmatize_text(travel_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_pred1 = nb.predict([travel_text])\n",
    "travel_pred2 = sgd.predict([travel_text])\n",
    "travel_pred3 = logreg.predict([travel_text])\n",
    "print(travel_pred1, travel_pred2, travel_pred3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "51d5ca9f4ad96dc911b2a44995fb8fda402c91559f770db7b0580424d28ceece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
